{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "[<< back to main index](../README.md)\n",
    "\n",
    "# Naive Bayes Spam Filtering\n",
    "\n",
    "### Overview\n",
    "\n",
    "We all hate spam, so developing a classifier solution to classify email as spam or not spam would be useful.  \n",
    "\n",
    "### Builds on\n",
    "None\n",
    "\n",
    "### Run time\n",
    "20-30 minutes.\n",
    "\n",
    "### Notes\n",
    "\n",
    "We will use TF-IDF to vectorize our texts, then NaiveBayes to classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, VectorAssembler\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Let's load the text data\n",
    "\n",
    "We will load the data into a Spark DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "R"
     ],
     "id": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 5,574 records in 9,220.83 ms\n",
      "+------+--------------------+\n",
      "|isspam|                text|\n",
      "+------+--------------------+\n",
      "|   ham|Go until jurong p...|\n",
      "|   ham|Ok lar... Joking ...|\n",
      "|  spam|Free entry in 2 a...|\n",
      "|   ham|U dun say so earl...|\n",
      "|   ham|Nah I don't think...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "\n",
    "data = spark.read.format(\"csv\").option('header','true').option('delimiter', '\\t').\\\n",
    "  option('inferSchema', 'true').load(\"/data/spam/SMSSpamCollection.txt\")\n",
    "\n",
    "t2 = time.perf_counter() \n",
    "\n",
    "print(\"read {:,} records in {:,.2f} ms\".format(data.count(), (t2-t1)*1000))\n",
    "data.show(5)\n",
    "# If you want to see full text, do\n",
    "# data.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|isspam|count|\n",
      "+------+-----+\n",
      "|   ham| 4827|\n",
      "|  spam|  747|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Count spam/ham\n",
    "data.groupby(\"isspam\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Convert each text entry to a vector of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|isspam|                text|               words|\n",
      "+------+--------------------+--------------------+\n",
      "|   ham|Go until jurong p...|[go, until, juron...|\n",
      "|   ham|Ok lar... Joking ...|[ok, lar..., joki...|\n",
      "|  spam|Free entry in 2 a...|[free, entry, in,...|\n",
      "|   ham|U dun say so earl...|[u, dun, say, so,...|\n",
      "|   ham|Nah I don't think...|[nah, i, don't, t...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each sentence into words\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(data)\n",
    "wordsData.printSchema\n",
    "wordsData.show(5)\n",
    "\n",
    "# If you want to see all words, do\n",
    "# wordsData.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+\n",
      "|isspam|                text|               words|         rawFeatures|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "|   ham|Go until jurong p...|[go, until, juron...|(1000,[7,77,150,1...|\n",
      "|   ham|Ok lar... Joking ...|[ok, lar..., joki...|(1000,[20,316,484...|\n",
      "|  spam|Free entry in 2 a...|[free, entry, in,...|(1000,[30,35,73,1...|\n",
      "|   ham|U dun say so earl...|[u, dun, say, so,...|(1000,[57,368,372...|\n",
      "|   ham|Nah I don't think...|[nah, i, don't, t...|(1000,[135,163,32...|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "featurizedData.show(5)\n",
    "\n",
    "# If you want to see word positions and TF frequencies, do\n",
    "# featurizedData.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Multiple TF by IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|isspam|            features|\n",
      "+------+--------------------+\n",
      "|   ham|(1000,[7,77,150,1...|\n",
      "|   ham|(1000,[20,316,484...|\n",
      "|  spam|(1000,[30,35,73,1...|\n",
      "|   ham|(1000,[57,368,372...|\n",
      "|   ham|(1000,[135,163,32...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"isspam\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Use StringIndexer to create a numeric label from the string column \"isspam.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|isspam|                text|               words|         rawFeatures|            features|label|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|   ham|Go until jurong p...|[go, until, juron...|(1000,[7,77,150,1...|(1000,[7,77,150,1...|  0.0|\n",
      "|   ham|Ok lar... Joking ...|[ok, lar..., joki...|(1000,[20,316,484...|(1000,[20,316,484...|  0.0|\n",
      "|  spam|Free entry in 2 a...|[free, entry, in,...|(1000,[30,35,73,1...|(1000,[30,35,73,1...|  1.0|\n",
      "|   ham|U dun say so earl...|[u, dun, say, so,...|(1000,[57,368,372...|(1000,[57,368,372...|  0.0|\n",
      "|   ham|Nah I don't think...|[nah, i, don't, t...|(1000,[135,163,32...|(1000,[135,163,32...|  0.0|\n",
      "|  spam|FreeMsg Hey there...|[freemsg, hey, th...|(1000,[25,36,68,9...|(1000,[25,36,68,9...|  1.0|\n",
      "|   ham|Even my brother i...|[even, my, brothe...|(1000,[18,47,48,5...|(1000,[18,47,48,5...|  0.0|\n",
      "|   ham|As per your reque...|[as, per, your, r...|(1000,[36,71,92,2...|(1000,[36,71,92,2...|  0.0|\n",
      "|  spam|WINNER!! As a val...|[winner!!, as, a,...|(1000,[39,43,61,7...|(1000,[39,43,61,7...|  1.0|\n",
      "|  spam|Had your mobile 1...|[had, your, mobil...|(1000,[36,73,82,1...|(1000,[36,73,82,1...|  1.0|\n",
      "|   ham|I'm gonna be home...|[i'm, gonna, be, ...|(1000,[26,41,106,...|(1000,[26,41,106,...|  0.0|\n",
      "|  spam|SIX chances to wi...|[six, chances, to...|(1000,[15,35,36,4...|(1000,[15,35,36,4...|  1.0|\n",
      "|  spam|URGENT! You have ...|[urgent!, you, ha...|(1000,[68,73,122,...|(1000,[68,73,122,...|  1.0|\n",
      "|   ham|I've been searchi...|[i've, been, sear...|(1000,[19,36,39,1...|(1000,[19,36,39,1...|  0.0|\n",
      "|   ham|I HAVE A DATE ON ...|[i, have, a, date...|(1000,[44,82,170,...|(1000,[44,82,170,...|  0.0|\n",
      "|  spam|XXXMobileMovieClu...|[xxxmobilemoviecl...|(1000,[41,43,49,6...|(1000,[41,43,49,6...|  1.0|\n",
      "|   ham|Oh k...i'm watchi...|[oh, k...i'm, wat...|(1000,[275,426,44...|(1000,[275,426,44...|  0.0|\n",
      "|   ham|Eh u remember how...|[eh, u, remember,...|(1000,[80,147,236...|(1000,[80,147,236...|  0.0|\n",
      "|   ham|Fine if thats th...|[fine, if, thats...|(1000,[80,159,170...|(1000,[80,159,170...|  0.0|\n",
      "|  spam|England v Macedon...|[england, v, mace...|(1000,[9,19,45,71...|(1000,[9,19,45,71...|  1.0|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "indexer = StringIndexer(inputCol=\"isspam\", outputCol=\"label\")\n",
    "indexed = indexer.fit(rescaledData).transform(rescaledData)\n",
    "indexed.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Split into Training and Test\n",
    "\n",
    "We will split our dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set count :  4465\n",
      "testing set count :  1109\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test\n",
    "(train, test) = indexed.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "print(\"training set count : \", train.count())\n",
    "print(\"testing set count : \", test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Fit Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained on 4,465 records  in 1,293.90 ms\n",
      "Train set accuracy = 0.9475923852183651\n",
      "+-----+----+---+\n",
      "|label|   0|  1|\n",
      "+-----+----+---+\n",
      "|  0.0|3678|199|\n",
      "|  1.0|  35|553|\n",
      "+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "t1 = time.perf_counter()\n",
    "model = nb.fit(train)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(\"trained on {:,} records  in {:,.2f} ms\".\\\n",
    "      format(train.count(), (t2-t1)*1000))\n",
    "\n",
    "# Confusion matrix on training data\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "train_fit = model.transform(train)\n",
    "accuracy = evaluator.evaluate(train_fit)\n",
    "print(\"Train set accuracy = \" + str(accuracy))\n",
    "train_fit.groupBy('label').pivot('prediction', [0,1]).count().na.fill(0).orderBy('label').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate the Model\n",
    "\n",
    "Let's look at how our model performs.  We will do an accuracy measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.9323715058611362\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(test_predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let us create a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "|label|  0|  1|\n",
      "+-----+---+---+\n",
      "|  0.0|886| 64|\n",
      "|  1.0| 11|148|\n",
      "+-----+---+---+\n",
      "\n",
      "1109\n"
     ]
    }
   ],
   "source": [
    "test_predictions.groupBy('label').pivot('prediction', [0,1]).count().na.fill(0).orderBy('label').show()\n",
    "\n",
    "## Can you explain the confusion matrix\n",
    "print(test_predictions.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Improve prediction results\n",
    "\n",
    "We used too few features above, and got bad accuracy. Increase the number of features for HashingTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 9:  Your own test\n",
    "\n",
    "Now it's your turn!   Make a new dataframe with some sample test data of your own creation.  Make some \"spammy\" SMSes and some ordinary ones.  See how our spam filter does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|               words|\n",
      "+--------------------+--------------------+\n",
      "|hey, can we meet ...|[hey,, can, we, m...|\n",
      "|WINNER!  Click he...|[winner!, , click...|\n",
      "|   CHEAP DEGREEES !!|[cheap, degreees,...|\n",
      "|      your text here|  [your, text, here]|\n",
      "|         FREE phones|      [free, phones]|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|               words|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|hey, can we meet ...|[hey,, can, we, m...|(1000,[238,486,74...|(1000,[238,486,74...|\n",
      "|WINNER!  Click he...|[winner!, , click...|(1000,[73,135,263...|(1000,[73,135,263...|\n",
      "|   CHEAP DEGREEES !!|[cheap, degreees,...|(1000,[119,339,66...|(1000,[119,339,66...|\n",
      "|      your text here|  [your, text, here]|(1000,[135,169,26...|(1000,[135,169,26...|\n",
      "|         FREE phones|      [free, phones]|(1000,[73,978],[1...|(1000,[73,978],[3...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: make a dataframe with some of your own data.\n",
    "mydata = pd.DataFrame({'text' : ['hey, can we meet 1 hr later?', \n",
    "                                'WINNER!  Click here to claim your FREE car !!!!',\n",
    "                                'CHEAP DEGREEES !!', \n",
    "                                'your text here',\n",
    "                                'FREE phones']\n",
    "                         })\n",
    "\n",
    "mydata2 = spark.createDataFrame(mydata)\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "fv = tokenizer.transform(mydata2)\n",
    "fv.show()\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "fv = hashingTF.transform(fv)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "fv = idfModel.transform(fv)\n",
    "fv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                text|prediction|\n",
      "+--------------------+----------+\n",
      "|hey, can we meet ...|       0.0|\n",
      "|WINNER!  Click he...|       0.0|\n",
      "|   CHEAP DEGREEES !!|       0.0|\n",
      "|      your text here|       1.0|\n",
      "|         FREE phones|       1.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(fv)\n",
    "predictions.select(['text', 'prediction']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUN : How will you defeat this algorithm? :-) \n",
    "\n",
    "If you are spammer, how can you defeat this algorithm?\n",
    "\n",
    "Some approaches\n",
    "- Find alternate words for spammy words (e.g   FREE --> no cost)\n",
    "- Misspell words : Winner --> w1nner,   FREE --> FR33\n",
    "\n",
    "<img src=\"../assets/images/come-tothe-dark-side-iin-we-have-cookies.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
