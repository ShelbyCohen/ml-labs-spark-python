{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines: College Admission\n",
    "\n",
    "Let's look at a classification example in Spark MLLib.  We looked at the college admission before. We can look again at this dataset.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI running on http://18.208.221.237:4040\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "print('Spark UI running on http://18.208.221.237:' + sc.uiWebUrl.split(':')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- admit: integer (nullable = true)\n",
      " |-- gre: integer (nullable = true)\n",
      " |-- gpa: double (nullable = true)\n",
      " |-- rank: integer (nullable = true)\n",
      "\n",
      "+-----+---+----+----+\n",
      "|admit|gre| gpa|rank|\n",
      "+-----+---+----+----+\n",
      "|    0|380|3.61|   3|\n",
      "|    1|660|3.67|   3|\n",
      "|    1|800| 4.0|   1|\n",
      "|    0|640|3.19|   4|\n",
      "|    0|520|2.93|   4|\n",
      "|    1|760| 3.0|   2|\n",
      "|    0|560|2.98|   1|\n",
      "|    0|400|3.08|   2|\n",
      "|    0|540|3.39|   3|\n",
      "|    1|700|3.92|   2|\n",
      "|    1|800| 4.0|   4|\n",
      "|    0|440|3.22|   1|\n",
      "|    1|760| 4.0|   1|\n",
      "|    1|700|3.08|   2|\n",
      "|    1|700| 4.0|   1|\n",
      "|    0|480|3.44|   3|\n",
      "|    1|780|3.87|   4|\n",
      "|    0|360|2.56|   3|\n",
      "|    1|800|3.75|   2|\n",
      "|    0|540|3.81|   1|\n",
      "+-----+---+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.read.csv(\"/data/college-admissions/admission-data.csv\", header=True, inferSchema=True)\n",
    "dataset.printSchema()\n",
    "dataset.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.43</td>\n",
       "      <td>600.0</td>\n",
       "      <td>3.390699999999998</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>0.49756985195624304</td>\n",
       "      <td>124.46248065545332</td>\n",
       "      <td>0.3971877275408833</td>\n",
       "      <td>1.019803902718557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                admit                 gre                 gpa  \\\n",
       "0   count                  100                 100                 100   \n",
       "1    mean                 0.43               600.0   3.390699999999998   \n",
       "2  stddev  0.49756985195624304  124.46248065545332  0.3971877275408833   \n",
       "3     min                    0                 300                2.42   \n",
       "4     max                    1                 800                 4.0   \n",
       "\n",
       "                rank  \n",
       "0                100  \n",
       "1               2.52  \n",
       "2  1.019803902718557  \n",
       "3                  1  \n",
       "4                  4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use describe\n",
    "dataset.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|admit|count|\n",
      "+-----+-----+\n",
      "|    1|   43|\n",
      "|    0|   57|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see data spread\n",
    "dataset.groupBy(\"admit\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build the Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+----+----------------+-----+\n",
      "|admit|gre| gpa|rank|        features|label|\n",
      "+-----+---+----+----+----------------+-----+\n",
      "|    0|640|3.19|   4|[640.0,3.19,4.0]|    0|\n",
      "|    0|360|2.56|   3|[360.0,2.56,3.0]|    0|\n",
      "|    0|520| 2.9|   3| [520.0,2.9,3.0]|    0|\n",
      "|    0|660|3.34|   3|[660.0,3.34,3.0]|    0|\n",
      "|    1|800|3.73|   1|[800.0,3.73,1.0]|    1|\n",
      "|    0|480|3.39|   4|[480.0,3.39,4.0]|    0|\n",
      "|    0|580| 4.0|   2| [580.0,4.0,2.0]|    0|\n",
      "|    1|500| 3.6|   3| [500.0,3.6,3.0]|    1|\n",
      "+-----+---+----+----+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "## TODO : input cols : gre, gpa, rank\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['gre', 'gpa', 'rank'], outputCol=\"features\")\n",
    "featureVector = assembler.transform(dataset)\n",
    "featureVector = featureVector.withColumn(\"label\", featureVector[\"admit\"])\n",
    "featureVector.sample(False, 0.1, seed=10).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split into training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set count  82\n",
      "testing set count  18\n"
     ]
    }
   ],
   "source": [
    "## Split into training and test\n",
    "## TODO: create training and test with an 80/20 split\n",
    "(training, test) = featureVector.randomSplit([80.0, 20.0])\n",
    "\n",
    "print (\"training set count \", training.count())\n",
    "print (\"testing set count \", test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build the Linear SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traind on 82 records in 1,982.28 ms\n",
      "inputs : gre, gpa, rank\n",
      "Coefficients: [0.003581719129981436,-0.25287701434842913,-0.4751690473427215]\n",
      "Intercept: -0.1752009294596929\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "## TODO : set MaxIter to 10\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the model\n",
    "t1 = time.perf_counter()\n",
    "## TODO : fit on 'training' set\n",
    "lsvcModel = lsvc.fit(training)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(\"traind on {:,} records in {:,.2f} ms\".\\\n",
    "      format(training.count(), (t2-t1)*1000))\n",
    "\n",
    "# Print the coefficients and intercept for linearsSVC\n",
    "print (\"inputs : gre, gpa, rank\")\n",
    "print(\"Coefficients: \" + str(lsvcModel.coefficients))\n",
    "print(\"Intercept: \" + str(lsvcModel.intercept))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust Iterations\n",
    "\n",
    "If any coefficient is zero, that variable won't be a factor in the decision !  \n",
    "Set **maxIter=50**  and try again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run the test set and get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+----+----------------+-----+--------------------+----------+\n",
      "|admit|gre| gpa|rank|        features|label|       rawPrediction|prediction|\n",
      "+-----+---+----+----+----------------+-----+--------------------+----------+\n",
      "|    0|440|3.13|   4|[440.0,3.13,4.0]|    0|[1.29142575654933...|       0.0|\n",
      "|    1|600| 3.4|   3| [600.0,3.4,3.0]|    1|[0.31145844228365...|       0.0|\n",
      "+-----+---+----+----+----------------+-----+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lsvcModel.transform(test)\n",
    "predictions.sample(False, 0.2).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: See the evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 - AUC\n",
    "**=> What does AUC mean?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7916666666666666"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)  #AUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 - Accuracy on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6111111111111112\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"accuracy\",  accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  7.3 - Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "|label|  0|  1|\n",
      "+-----+---+---+\n",
      "|    0|  7|  5|\n",
      "|    1|  2|  4|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "predictions.groupBy('label').pivot('prediction', [0,1]).count().na.fill(0).orderBy('label').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO: What is the meaning of the confusion matrix?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase 'maxIterations' and try again\n",
    "In **Step-5** increase 'maxIter' to 50 and run again.  \n",
    "Does the accuracy go up?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 8: Try running a prediction on your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gre  gpa  rank\n",
      "0  600  4.0     1\n",
      "1  700  3.5     2\n",
      "2  800  3.2     3\n",
      "+---+---+----+---------------+--------------------+----------+\n",
      "|gre|gpa|rank|       features|       rawPrediction|prediction|\n",
      "+---+---+----+---------------+--------------------+----------+\n",
      "|600|4.0|   1|[600.0,4.0,1.0]|[-0.4871534437927...|       1.0|\n",
      "|700|3.5|   2|[700.0,3.5,2.0]|[-0.4965948166223...|       1.0|\n",
      "|800|3.2|   3|[800.0,3.2,3.0]|[-0.4554607865823...|       1.0|\n",
      "+---+---+----+---------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdata = pd.DataFrame({'gre' : [600, 700, 800], \n",
    "                        'gpa' : [4.0, 3.5, 3.2],\n",
    "                        'rank': [1,   2,   3]}\n",
    "             )\n",
    "print(newdata)\n",
    "\n",
    "## TODO : create a spark dataframe\n",
    "## Hint : input is 'newdata'\n",
    "spark_newdata = spark.createDataFrame(newdata)\n",
    "\n",
    "## TODO : create feature vector\n",
    "## Hint : spark_newdata\n",
    "newfeatures = assembler.transform(spark_newdata)\n",
    "\n",
    "lsvcModel.transform(newfeatures).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Understanding the impact of Scaling Data\n",
    "Just now we have fed our input vector without scaling to SVM.  \n",
    "IN this section we are going to scale the data and see if it improves the prediction.  \n",
    "We will condense the code to focus on important stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 : Raw data (without scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== run with raw data (not scaled) =======\n",
      "inputs :  gre, gpa, rank\n",
      "Coefficients: [0.004471158074653798,1.2161147145938713,-0.08283634499589466]\n",
      "Intercept: -6.953627550839707\n",
      "accuracy 0.8181818181818182\n",
      "===== END run with raw data (not scaled) =======\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "\n",
    "print (\"===== run with raw data (not scaled) =======\")\n",
    "dataset = spark.read.csv(\"/data/college-admissions/admission-data.csv\", \\\n",
    "                         header=True, inferSchema=True)\n",
    "assembler = VectorAssembler(inputCols=[ 'gre', 'gpa', 'rank'], outputCol=\"features\")\n",
    "featureVector = assembler.transform(dataset)\n",
    "featureVector = featureVector.withColumn(\"label\", featureVector[\"admit\"])\n",
    "(training, test) = featureVector.randomSplit([0.8, 0.2], seed=123)\n",
    "lsvc = LinearSVC(maxIter=100, regParam=0.3, featuresCol='features')\n",
    "lsvcModel = lsvc.fit(training)\n",
    "print (\"inputs :  gre, gpa, rank\")\n",
    "print(\"Coefficients: \" + str(lsvcModel.coefficients))\n",
    "print(\"Intercept: \" + str(lsvcModel.intercept))\n",
    "predictions = lsvcModel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)  #AUC\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"accuracy\",  accuracy)\n",
    "print (\"===== END run with raw data (not scaled) =======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 : Scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== run with scaled data  =======\n",
      "+-----+---+----+----+----------------+-----+---------------------------------------------------------+\n",
      "|admit|gre|gpa |rank|features        |label|featuresScaled                                           |\n",
      "+-----+---+----+----+----------------+-----+---------------------------------------------------------+\n",
      "|0    |380|3.61|3   |[380.0,3.61,3.0]|0    |[3.05312892687673,9.088901166082522,2.9417420270727606]  |\n",
      "|1    |660|3.67|3   |[660.0,3.67,3.0]|1    |[5.302802872996426,9.23996323532489,2.9417420270727606]  |\n",
      "|1    |800|4.0 |1   |[800.0,4.0,1.0] |1    |[6.427639846056274,10.070804616157918,0.9805806756909201]|\n",
      "|0    |640|3.19|4   |[640.0,3.19,4.0]|0    |[5.14211187684502,8.03146668138594,3.9223227027636804]   |\n",
      "|0    |520|2.93|4   |[520.0,2.93,4.0]|0    |[4.177965899936578,7.376864381335675,3.9223227027636804] |\n",
      "|1    |760|3.0 |2   |[760.0,3.0,2.0] |1    |[6.10625785375346,7.553103462118439,1.9611613513818402]  |\n",
      "|0    |560|2.98|1   |[560.0,2.98,1.0]|0    |[4.499347892239392,7.5027494390376495,0.9805806756909201]|\n",
      "|0    |400|3.08|2   |[400.0,3.08,2.0]|0    |[3.213819923028137,7.754519554441598,1.9611613513818402] |\n",
      "|0    |540|3.39|3   |[540.0,3.39,3.0]|0    |[4.3386568960879845,8.535006912193836,2.9417420270727606]|\n",
      "|1    |700|3.92|2   |[700.0,3.92,2.0]|1    |[5.62418486529924,9.86938852383476,1.9611613513818402]   |\n",
      "|1    |800|4.0 |4   |[800.0,4.0,4.0] |1    |[6.427639846056274,10.070804616157918,3.9223227027636804]|\n",
      "|0    |440|3.22|1   |[440.0,3.22,1.0]|0    |[3.535201915330951,8.106997716007125,0.9805806756909201] |\n",
      "|1    |760|4.0 |1   |[760.0,4.0,1.0] |1    |[6.10625785375346,10.070804616157918,0.9805806756909201] |\n",
      "|1    |700|3.08|2   |[700.0,3.08,2.0]|1    |[5.62418486529924,7.754519554441598,1.9611613513818402]  |\n",
      "|1    |700|4.0 |1   |[700.0,4.0,1.0] |1    |[5.62418486529924,10.070804616157918,0.9805806756909201] |\n",
      "|0    |480|3.44|3   |[480.0,3.44,3.0]|0    |[3.8565839076337642,8.66089196989581,2.9417420270727606] |\n",
      "|1    |780|3.87|4   |[780.0,3.87,4.0]|1    |[6.266948849904867,9.743503466132786,3.9223227027636804] |\n",
      "|0    |360|2.56|3   |[360.0,2.56,3.0]|0    |[2.892437930725323,6.445314954341068,2.9417420270727606] |\n",
      "|1    |800|3.75|2   |[800.0,3.75,2.0]|1    |[6.427639846056274,9.441379327648049,1.9611613513818402] |\n",
      "|0    |540|3.81|1   |[540.0,3.81,1.0]|0    |[4.3386568960879845,9.592441396890417,0.9805806756909201]|\n",
      "+-----+---+----+----+----------------+-----+---------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "inputs :  gre, gpa, rank\n",
      "Coefficients: [0.556491425374074,0.4830258399185652,-0.0844768279137541]\n",
      "Intercept: -6.95362755084178\n",
      "accuracy 0.8181818181818182\n",
      "===== END run with scaled data =======\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "print (\"===== run with scaled data  =======\")\n",
    "dataset = spark.read.csv(\"/data/college-admissions/admission-data.csv\", \\\n",
    "                         header=True, inferSchema=True)\n",
    "assembler = VectorAssembler(inputCols=[ 'gre', 'gpa', 'rank'], outputCol=\"features\")\n",
    "featureVector = assembler.transform(dataset)\n",
    "featureVector = featureVector.withColumn(\"label\", featureVector[\"admit\"])\n",
    "\n",
    "# scaling\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"featuresScaled\",\n",
    "                        withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(featureVector)\n",
    "fv_scaled = scalerModel.transform(featureVector)\n",
    "fv_scaled.show(20, truncate=False)\n",
    "\n",
    "(training, test) = fv_scaled.randomSplit([0.8, 0.2], seed=123)  ## CHANGED\n",
    "lsvc = LinearSVC(maxIter=100, regParam=0.3, featuresCol='featuresScaled')  ## CHANGED\n",
    "\n",
    "lsvcModel = lsvc.fit(training)\n",
    "print (\"inputs :  gre, gpa, rank\")\n",
    "print(\"Coefficients: \" + str(lsvcModel.coefficients))\n",
    "print(\"Intercept: \" + str(lsvcModel.intercept))\n",
    "predictions = lsvcModel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)  #AUC\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"accuracy\",  accuracy)\n",
    "print (\"===== END run with scaled data =======\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 : Discuss the findings\n",
    "\n",
    "### Coefficients\n",
    "Here are the coefficients from one sample run\n",
    "\n",
    "raw data run : \n",
    "inputs :  gre, gpa, rank\n",
    "Coefficients: [0.00730924862823,0.803788881405,-0.182571791707]\n",
    "Intercept: -7.016411699283878\n",
    "\n",
    "scaled data run:\n",
    "inputs :  gre, gpa, rank\n",
    "Coefficients: [0.985025239033,0.311565356517,-0.180265498388]\n",
    "Intercept: -7.500693768967119\n",
    "\n",
    "### Accuracy\n",
    "Compare accuracies"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
